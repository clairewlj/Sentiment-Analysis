{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#import csv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pandas read csv\n",
    "#read data\n",
    "train_df = pd.read_csv(\"/Users/ClaireWang/Documents/study/previous/STOR 767/train.csv\",header = 0, encoding = 'ISO-8859-1')\n",
    "test_df = pd.read_csv(\"/Users/ClaireWang/Documents/study/previous/STOR 767/test.csv\",header = 0, encoding = 'ISO-8859-1')\n",
    "#train_df.head(5)\n",
    "#test_df.head(5)\n",
    "\n",
    "train_df.columns = ['text','sentiment']\n",
    "test_df.columns = ['text','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data processing - tokenization, stop words removal, non-letters removal, stemming\n",
    "\n",
    "#Pandas DataFrame columns are Pandas Series when you pull them out, \n",
    "#which you can then call .tolist() on to turn them into a Python list\n",
    "train_text = train_df.text.tolist()\n",
    "test_text = test_df.text.tolist()\n",
    "\n",
    "train_sentiment = train_df.sentiment\n",
    "test_sentiment = test_df.sentiment\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    #remove non letters\n",
    "    text = re.sub('[^a-zA-Z0-9]',' ',text)\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #stem\n",
    "    stems = stem_tokens(tokens,stemmer)\n",
    "    return stems\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "    train_text[i] = train_text[i].lstrip('\"').rstrip().lower()\n",
    "    train_text[i] = ' '.join(tokenize(train_text[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #f read\n",
    "# #read data\n",
    "# with open(\"/Users/ClaireWang/Documents/study/previous/STOR 767/train.csv\",encoding=\"ISO-8859-1\") as f:\n",
    "#     f_train = f.readlines()[1:]\n",
    "# with open(\"/Users/ClaireWang/Documents/study/previous/STOR 767/test.csv\",encoding=\"ISO-8859-1\") as f:\n",
    "#     f_test = f.readlines()[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    min_df=5\n",
    "    #max_df=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_t = TfidfVectorizer(\n",
    "    min_df=5,\n",
    "    stop_words = 'english',\n",
    "    lowercase = True,\n",
    "    tokenizer = tokenize,\n",
    "    analyzer='word',\n",
    "    sublinear_tf = True\n",
    "    #max_df = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform data into feature vectors & fit the model\n",
    "corpus = train_text\n",
    "corpus2 = test_text \n",
    "\n",
    "#The method fit_transform does two functions: First, it fits the model and learns the vocabulary; \n",
    "#second, it transforms our corpus data into feature vectors. \n",
    "#The input to fit_transform should be a list of strings, so we concatenate train and test data as follows.\n",
    "\n",
    "#CountVectorizer\n",
    "corpus_data_features = vectorizer.fit_transform(corpus)\n",
    "corpus_data_features2 = vectorizer.transform(corpus2)\n",
    "#Numpy arrays are easy to work with, so convert the result to an array.\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "corpus_data_features_nd2 = corpus_data_features2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfIDF\n",
    "corpus_data_features_t = vectorizer_t.fit_transform(corpus)\n",
    "corpus_data_features2_t = vectorizer_t.transform(corpus2)\n",
    "#Numpy arrays are easy to work with, so convert the result to an array.\n",
    "corpus_data_features_nd_t = corpus_data_features_t.toarray()\n",
    "corpus_data_features_nd2_t = corpus_data_features2_t.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check features and the counts\n",
    "#vectorizer._validate_vocabulary()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "    \n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the data set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression - CountVectorizer\n",
    "#train classifer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "t0=time.time()\n",
    "log_model = log_model.fit(X=corpus_data_features_nd,y=train_sentiment)\n",
    "t1=time.time()\n",
    "#label evaluation set.We can use either predict for classes or predict_proba for probabilities.\n",
    "y_pred = log_model.predict(corpus_data_features_nd2)\n",
    "t2=time.time()\n",
    "time_lr_train = t1-t0\n",
    "time_lr_predict = t2-t1\n",
    "\n",
    "#There is a function for classification called sklearn.metrics.classification_report\n",
    "#which calculates several types of (predictive) scores on a classification model.\n",
    "\n",
    "print(\"Results for Logistic Regression - CountVectorizer\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_lr_train, time_lr_predict))\n",
    "print(classification_report(test_sentiment,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression - TFIDF Vectorizer\n",
    "#train classifer\n",
    "log_model = LogisticRegression()\n",
    "t0=time.time()\n",
    "log_model = log_model.fit(X=corpus_data_features_nd_t,y=train_sentiment)\n",
    "t1=time.time()\n",
    "#label evaluation set.We can use either predict for classes or predict_proba for probabilities.\n",
    "y_pred = log_model.predict(corpus_data_features_nd2_t)\n",
    "t2=time.time()\n",
    "time_lr_train = t1-t0\n",
    "time_lr_predict = t2-t1\n",
    "\n",
    "#There is a function for classification called sklearn.metrics.classification_report\n",
    "#which calculates several types of (predictive) scores on a classification model.\n",
    "\n",
    "print(\"Results for Logistic Regression - TFIDF\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_lr_train, time_lr_predict))\n",
    "print(classification_report(test_sentiment,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM, kernel=RBF - CountVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "CountV_train = corpus_data_features_nd\n",
    "CountV_test = corpus_data_features_nd2\n",
    "\n",
    "#SVM with kernel = RBF\n",
    "classifier_rbf = svm.SVC()\n",
    "t0=time.time()\n",
    "classifier_rbf.fit(CountV_train,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_rbf = classifier_rbf.predict(CountV_test)\n",
    "t2=time.time()\n",
    "time_rbf_train = t1-t0\n",
    "time_rbf_predict = t2-t1\n",
    "\n",
    "#Print results\n",
    "print(\"Results for SVC(kernel=rbf) - CountVectorizer\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
    "print(classification_report(test_sentiment, prediction_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM, kernel=RBF - TFIDF Vectorizer\n",
    "CountV_train_t = corpus_data_features_nd_t\n",
    "CountV_test_t = corpus_data_features_nd2_t\n",
    "\n",
    "#SVM with kernel = RBF\n",
    "classifier_rbf = svm.SVC()\n",
    "t0=time.time()\n",
    "classifier_rbf.fit(CountV_train_t,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_rbf = classifier_rbf.predict(CountV_test_t)\n",
    "t2=time.time()\n",
    "time_rbf_train = t1-t0\n",
    "time_rbf_predict = t2-t1\n",
    "\n",
    "#Print results\n",
    "print(\"Results for SVC(kernel=rbf) - TFIDF\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
    "print(classification_report(test_sentiment, prediction_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM with kernel = linear 1 \n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0=time.time()\n",
    "classifier_linear.fit(CountV_train,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_linear = classifier_linear.predict(CountV_test)\n",
    "t2=time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "print(\"Results for SVC(kernel=linear)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "print(classification_report(test_sentiment, prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM with kernel = linear 1 - TFIDF\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0=time.time()\n",
    "classifier_linear.fit(CountV_train_t,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_linear = classifier_linear.predict(CountV_test_t)\n",
    "t2=time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "print(\"Results for SVC(kernel=linear) - TFIDF\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "print(classification_report(test_sentiment, prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM with kernel = linear 2\n",
    "classifier_liblinear = svm.LinearSVC()\n",
    "t0=time.time()\n",
    "classifier_liblinear.fit(CountV_train,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_liblinear = classifier_liblinear.predict(CountV_test)\n",
    "t2=time.time()\n",
    "time_liblinear_train = t1-t0\n",
    "time_liblinear_predict = t2-t1\n",
    "\n",
    "print(\"Results for LinearSVC()\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_liblinear_train, time_liblinear_predict))\n",
    "print(classification_report(test_sentiment, prediction_liblinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM with kernel = linear 2 - TFIDF\n",
    "classifier_liblinear = svm.LinearSVC()\n",
    "t0=time.time()\n",
    "classifier_liblinear.fit(CountV_train_t,train_df.sentiment)\n",
    "t1=time.time()\n",
    "prediction_liblinear = classifier_liblinear.predict(CountV_test_t)\n",
    "t2=time.time()\n",
    "time_liblinear_train = t1-t0\n",
    "time_liblinear_predict = t2-t1\n",
    "\n",
    "print(\"Results for LinearSVC() - TFIDF\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_liblinear_train, time_liblinear_predict))\n",
    "print(classification_report(test_sentiment, prediction_liblinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes - CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_gnb = GaussianNB()\n",
    "t0=time.time()\n",
    "classifier_gnb.fit(CountV_train,train_sentiment)\n",
    "t1=time.time()\n",
    "prediction_gnb = classifier_gnb.predict(CountV_test)\n",
    "t2=time.time()\n",
    "time_gnb_train=t1-t0\n",
    "time_gnb_predict=t2-t1\n",
    "\n",
    "print(\"Results for Gaussian Naive Bayes\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_gnb_train, time_gnb_predict))\n",
    "print(classification_report(test_sentiment, prediction_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes - TFIDF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_gnb = GaussianNB()\n",
    "t0=time.time()\n",
    "classifier_gnb.fit(CountV_train_t,train_sentiment)\n",
    "t1=time.time()\n",
    "prediction_gnb = classifier_gnb.predict(CountV_test_t)\n",
    "t2=time.time()\n",
    "time_gnb_train=t1-t0\n",
    "time_gnb_predict=t2-t1\n",
    "\n",
    "print(\"Results for Gaussian Naive Bayes - TFIDF\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_gnb_train, time_gnb_predict))\n",
    "print(classification_report(test_sentiment, prediction_gnb))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
